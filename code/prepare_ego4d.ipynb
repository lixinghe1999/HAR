{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ego4d.ego4d_dataset import Ego4D_Narration\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text embedding for capture24\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "capture24 = pd.read_csv('./resources/capture24_label_count.csv')\n",
    "clean_annotation = []\n",
    "for annotation, count in zip(capture24['annotation'], capture24['count']):\n",
    "    annotations = annotation.split(';')\n",
    "    annotation = re.sub(r'\\d+', '', annotations[-2])\n",
    "    clean_annotation.append(annotation)\n",
    "\n",
    "embeddings = model.encode(clean_annotation)\n",
    "np.save('./resources/capture24_label_embedding.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Ego4D_Narration(modal=['audio', 'imu'], window_sec=10)\n",
    "all_embeddings = {'audio': [], 'imu': [], 'text': []}\n",
    "from imagebind import data\n",
    "import torch\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully.\")\n",
    "for idx in range(len(dataset)):\n",
    "    data_sample = dataset.__getitem__(idx)\n",
    "    imu = torch.from_numpy(data_sample['imu'])[None, :]\n",
    "    audio = torch.from_numpy(data_sample['audio'][None, :])\n",
    "    text = data_sample['text']\n",
    "    inputs = {\n",
    "        ModalityType.IMU: imu.float().to(device),\n",
    "        ModalityType.AUDIO: data.load_and_transform_audio_data([[audio, 16000]], (device)),\n",
    "        ModalityType.TEXT: data.load_and_transform_text([text], device),\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "    imu_embedding = embeddings[ModalityType.IMU].cpu().numpy()\n",
    "    audio_embedding = embeddings[ModalityType.AUDIO].cpu().numpy()\n",
    "    text_embedding = embeddings[ModalityType.TEXT].cpu().numpy()\n",
    "    all_embeddings['audio'].append(audio_embedding)\n",
    "    all_embeddings['imu'].append(imu_embedding)\n",
    "    all_embeddings['text'].append(text_embedding)\n",
    "    break\n",
    "np.save('./resources/ego4d_imagebind_audio_embedding.npy', np.concatenate(all_embeddings['audio'], axis=0))\n",
    "np.save('./resources/ego4d_imagebind_imu_embedding.npy', np.concatenate(all_embeddings['imu'], axis=0))\n",
    "np.save('./resources/ego4d_imagebind_text_embedding.npy', np.concatenate(all_embeddings['text'], axis=0))\n",
    "\n",
    "motion_candidate = ['walking', 'standing', 'sports', 'lying down', 'sitting', 'transportation']\n",
    "inputs = {\n",
    "        ModalityType.TEXT: data.load_and_transform_text(motion_candidate, device),\n",
    "}\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "np.save('./resources/ego4d_imagebind_motion_embedding.npy', embeddings[ModalityType.TEXT].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding visualization\n",
    "dataset = Ego4D_Narration(modal=['audio', 'imu'], window_sec=10)\n",
    "scenario_embeddings = {}; scenario_labels = []\n",
    "for i in range(len(dataset)):\n",
    "    data_sample = dataset.window_idx[i]\n",
    "    scenario = data_sample['scenario']\n",
    "    for s in scenario:\n",
    "        if s not in scenario_embeddings:\n",
    "            scenario_embeddings[s] = []\n",
    "        scenario_embeddings[s].append(i)\n",
    "\n",
    "audio_embeddings = np.load('./resources/ego4d_imagebind_audio_embedding.npy')\n",
    "imu_embeddings = np.load('./resources/ego4d_imagebind_imu_embedding.npy')\n",
    "text_embeddings = np.load('./resources/ego4d_imagebind_text_embedding.npy')\n",
    "\n",
    "random_scenario = np.random.choice(list(scenario_embeddings.keys()), 10, replace=False)\n",
    "all_embeddings = {'audio': [], 'imu': [], 'text': []}; all_labels = []\n",
    "for i, scenario in enumerate(random_scenario):\n",
    "    indices = scenario_embeddings[scenario]\n",
    "    random_indices = np.random.choice(indices, 50, replace=False)\n",
    "    all_embeddings['audio'].append(audio_embeddings[random_indices])\n",
    "    all_embeddings['imu'].append(imu_embeddings[random_indices])\n",
    "    all_embeddings['text'].append(text_embeddings[random_indices])\n",
    "    all_labels.extend([i] * 50)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "for modality in all_embeddings:\n",
    "    embeddings_2d = tsne.fit_transform(all_embeddings[modality])\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=all_labels)\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./figs/ego4d_{modality}_scenario_mapping.png')\n",
    "\n",
    "embeddings_2d = tsne.fit_transform(np.concatenate([all_embeddings['audio'], all_embeddings['imu']], axis=1))\n",
    "fig, axs = plt.subplots(1, 1, figsize=(6, 4))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=all_labels)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./figs/ego4d_multimodal_scenario_mapping.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41130835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario similarity\n",
    "dataset = Ego4D_Narration(modal=['audio', 'imu'], window_sec=10)\n",
    "text_embeddings = np.load('./resources/ego4d_narration_embedding.npy')\n",
    "\n",
    "scenario_embeddings = {}\n",
    "for i in range(len(dataset)):\n",
    "    data_sample = dataset.window_idx[i]\n",
    "    scenario = data_sample['scenario']\n",
    "    for s in scenario:\n",
    "        if s not in scenario_embeddings:\n",
    "            scenario_embeddings[s] = []\n",
    "        scenario_embeddings[s].append(i)\n",
    "scenario_names = list(scenario_embeddings.keys())\n",
    "scenario_similarities = []\n",
    "for s in scenario_names:\n",
    "    scenario = dataset.scenario_map[s].replace('/', ', ')\n",
    "    idxs = scenario_embeddings[s]\n",
    "    idxs = np.random.choice(idxs, 500)\n",
    "    text_embeddings_scenario = text_embeddings[idxs]\n",
    "    all_idxs = np.arange(len(dataset))\n",
    "    random_idxs = np.random.choice(all_idxs, len(idxs), replace=False)\n",
    "    random_embeddings = text_embeddings[random_idxs]\n",
    "\n",
    "    intra_cosine_similarity = np.dot(text_embeddings_scenario, text_embeddings_scenario.T)\n",
    "    intra_cosine_similarity = np.mean(intra_cosine_similarity)\n",
    "\n",
    "    inter_cosine_similarity = np.dot(text_embeddings_scenario, random_embeddings.T)\n",
    "    inter_cosine_similarity = np.mean(inter_cosine_similarity)\n",
    "    scenario_similarities.append([intra_cosine_similarity, inter_cosine_similarity])\n",
    "np.savetxt('./resources/ego4d_scenario_similarity.txt', scenario_similarities, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55798e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given activity_set, find the top 50 samples for each activity\n",
    "dataset = Ego4D_Narration(modal=['audio', 'imu'], window_sec=10)\n",
    "text_embeddings = np.load('./resources/ego4d_narration_embedding.npy')\n",
    "audio_embeddings = np.load('./resources/ego4d_audio_embedding.npy')\n",
    "\n",
    "import json\n",
    "activity_motion_attributes = json.load(open('./resources/ego4d_activity_basic.json'))\n",
    "activity_candidates = list(activity_motion_attributes.keys())\n",
    "\n",
    "import laion_clap   \n",
    "model = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "model.load_ckpt()\n",
    "activity_embeddings = model.get_text_embedding(activity_candidates)\n",
    "cosine_similarity = np.dot(text_embeddings, activity_embeddings.T)\n",
    "\n",
    "# for each activity, find the top 50 samples\n",
    "top_k = 50\n",
    "dataset_folder = '../dataset/ego4d/sampled/'\n",
    "for i in range(len(activity_candidates)):\n",
    "    argmax_activity = activity_candidates[i]\n",
    "    idxs = np.argsort(cosine_similarity[:, i])[-top_k:]\n",
    "    average_similarity = np.mean(cosine_similarity[idxs, i])\n",
    "    print(average_similarity)\n",
    "    # only keep the idxs with the > 0.7\n",
    "    idxs = idxs[np.where(cosine_similarity[idxs, i] > 0.7)]\n",
    "    print(argmax_activity, len(idxs))\n",
    "    dataset_folder_activity = f'../dataset/ego4d/mini/{argmax_activity}'\n",
    "    os.makedirs(dataset_folder_activity, exist_ok=True)\n",
    "    for j, idx in enumerate(idxs):\n",
    "        imu_activity = ', '.join(activity_motion_attributes[argmax_activity])\n",
    "        data_sample = dataset.window_idx[idx]\n",
    "        scenario = data_sample['scenario']\n",
    "        if len(scenario) > 3: # meaningless scenario\n",
    "            continue\n",
    "        scenario = ', '.join([dataset.scenario_map[s].replace('/', ' or ') for s in scenario])\n",
    "        text = data_sample['text']\n",
    "        audio_name = f'{dataset_folder_activity}/{j}_{scenario}_{argmax_activity}_{text}_{imu_activity}.wav'\n",
    "        imu_name = audio_name.replace('.wav', '.npy')\n",
    "\n",
    "        data_sample = dataset.__getitem__(idx)\n",
    "        audio = data_sample['audio']\n",
    "        imu = data_sample['imu']\n",
    "        np.save(imu_name, imu[:])\n",
    "        sf.write(audio_name, audio, 16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "har",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
